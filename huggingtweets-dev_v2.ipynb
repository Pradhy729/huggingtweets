{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingTweets - Tweet Generation with Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation (NLG).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Generating realistic text has become more and more efficient with models such as [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). Those models are trained on very large datasets and require heavy computer resources (and time!).\n",
    "\n",
    "However, we can use Transfer Learning and a single GPU to quickly fine-tune a pre-trained model on a given task.\n",
    "\n",
    "We test if we can imitate the writing style of a Twitter user by only using some of his tweets. Twitter API let us download \"only\" the 3200 most recent tweets from any single user, which we then filter out (to remove retweets, short content, etc).\n",
    "\n",
    "[HuggingFace](https://huggingface.co/) gives us an easy access to pre-trained models and fine-tuning techniques for Natural Language Generation (NLG) tasks.\n",
    "\n",
    "We will be monitoring the training with [W&B](https://docs.wandb.com/huggingface) (which is integrated in HuggingFace) to ensure the model is learning from the data and compare multiple experiments.\n",
    "\n",
    "![](https://i.imgur.com/vnejHGh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if those libraries are not installed\n",
    "#!source ~/aliases.zsh\n",
    "#!pip install --proxy https://proxy-chain.intel.com:911 wandb -qq\n",
    "#!pip install --proxy https://proxy-chain.intel.com:911 tweepy -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface scripts for fine-tuning models and language generation\n",
    "#!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py -q\n",
    "#!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/text-generation/run_generation.py -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a Twitter Development Account\n",
    "\n",
    "In order to access Twitter data, we need to:\n",
    "\n",
    "* [create a Twitter development account](https://developer.twitter.com/en/apply-for-access)\n",
    "* [create a Twitter app](https://developer.twitter.com/en/apps)\n",
    "* get your consumer API keys: \"API key\" and \"API secret key\"\n",
    "\n",
    "The entire process only takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('credentials.json','r') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download tweets from a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download latest tweets associated to a user account through [Tweepy](http://docs.tweepy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate\n",
    "auth = tweepy.AppAuthHandler(credentials.get('api_key'), credentials.get('api_secret'))\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grab all available tweets (limited to 3200 per API limitations) based on Twitter handle.\n",
    "\n",
    "**Note**: Protected users may only be requested when the authenticated user either \"owns\" the timeline or is an approved follower of the owner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <--- Enter the screen name of the user you will download your dataset from --->\n",
    "handle = 'PradhyumnaR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before id 1135341235948929030\n",
      "...394 tweets downloaded so far\n",
      "getting tweets before id 985234483199721471\n",
      "...591 tweets downloaded so far\n",
      "getting tweets before id 822930581914693631\n",
      "...776 tweets downloaded so far\n",
      "getting tweets before id 722876081569538047\n",
      "...943 tweets downloaded so far\n",
      "getting tweets before id 655364240992169984\n",
      "...1129 tweets downloaded so far\n",
      "getting tweets before id 531100699430371327\n",
      "...1324 tweets downloaded so far\n",
      "getting tweets before id 448493826433228801\n",
      "...1520 tweets downloaded so far\n",
      "getting tweets before id 359686179043557375\n",
      "...1558 tweets downloaded so far\n",
      "getting tweets before id 259324135505793023\n",
      "...1558 tweets downloaded so far\n",
      "getting tweets before id 259324135505793023\n",
      "...1558 tweets downloaded so far\n",
      "getting tweets before id 259324135505793023\n",
      "...1558 tweets downloaded so far\n",
      "getting tweets before id 259324135505793023\n",
      "...1558 tweets downloaded so far\n",
      "getting tweets before id 259324135505793023\n",
      "...1558 tweets downloaded so far\n",
      "getting tweets before id 259324135505793023\n"
     ]
    }
   ],
   "source": [
    "# Adapted from https://gist.github.com/onmyeoin/62c72a7d61fc840b2689b2cf106f583c\n",
    "\n",
    "# initialize a list to hold all the tweepy Tweets & list with no retweets\n",
    "alltweets = []\n",
    "\n",
    "# make initial request for most recent tweets with extended mode enabled to get full tweets\n",
    "new_tweets = api.user_timeline(\n",
    "    screen_name=handle, tweet_mode='extended', count=200)\n",
    "\n",
    "# save most recent tweets\n",
    "alltweets.extend(new_tweets)\n",
    "\n",
    "# save the id of the oldest tweet less one\n",
    "oldest = alltweets[-1].id - 1\n",
    "\n",
    "# check we cannot get more tweets\n",
    "no_tweet_count = 0\n",
    "\n",
    "# keep grabbing tweets until the api limit is reached\n",
    "while True:\n",
    "    print(f'getting tweets before id {oldest}')\n",
    "\n",
    "    # all subsequent requests use the max_id param to prevent duplicates\n",
    "    new_tweets = api.user_timeline(\n",
    "        screen_name=handle, tweet_mode='extended', count=200, max_id=oldest)\n",
    "    \n",
    "    # stop if no more tweets (try a few times as they sometimes eventually come)\n",
    "    if not new_tweets:\n",
    "        no_tweet_count +=1\n",
    "    else:\n",
    "        no_tweet_count = 0\n",
    "    if no_tweet_count > 5: break\n",
    "\n",
    "    # save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    # update the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    print(f'...{len(alltweets)} tweets downloaded so far')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset from downloaded tweets\n",
    "\n",
    "We remove:\n",
    "* retweets (since it's not in the wording style of target author)\n",
    "* tweets with no interesting content (limited to url's, user mentionss, \"thank you\"…)\n",
    "\n",
    "We clean up remaining tweets:\n",
    "* we remove url's\n",
    "* we replace \"@\" mentions with user names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_handle:\n",
    "    'Get a user handle and cache it to avoid calling too much twitter api.'\n",
    "    handles = {}\n",
    "    def get(handle):\n",
    "        if handle not in user_handle.handles.keys():            \n",
    "            try:\n",
    "                user_handle.handles[handle] = api.get_user(handle).name\n",
    "            except:\n",
    "                user_handle.handles[handle] = None\n",
    "        return user_handle.handles[handle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_handle(word):\n",
    "    'Replace user handles, remove \"@\" and \"#\"'\n",
    "    if word[0] == '@':\n",
    "        handle = re.search('^@(\\w)+', word)\n",
    "        if handle:\n",
    "            user = user_handle.get(handle.group())\n",
    "            if user is not None: return user + word[handle.endpos:]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_tweet(tweet):\n",
    "    'Return true if not a retweet'\n",
    "    if hasattr(tweet, 'retweeted_status'):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_tweets(tweets):\n",
    "    'Decide which tweets we keep and replace handles'\n",
    "    curated_tweets = []\n",
    "    for tweet in tweets:\n",
    "        if keep_tweet(tweet):\n",
    "            curated_tweets.append(' '.join(replace_handle(w) for w in tweet.full_text.split()))\n",
    "    return curated_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_tweets = curate_tweets(alltweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify our list of tweets is well curated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 1558\n",
      "Curated tweets: 491\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of tweets: {len(alltweets)}\\nCurated tweets: {len(curated_tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweets\n",
      "\n",
      "@omarsar0 A project to build an end to end nlp project would be great. Even better if students can collaborate on it.\n",
      "\n",
      "RT @thehill: Dorsey defends decision to fact check Trump tweet: \"More transparency from us is critical\" https://t.co/mJFDpsXosa https://t.c…\n",
      "\n",
      "RT @DaleJohnsonESPN: Premier League returning on Wednesday, June 17 with the two games in hand, and in full from June 20-21, leaves ample t…\n",
      "\n",
      "RT @NYGovCuomo: Now is the time for government to stimulate the economy by investing in infrastructure.\n",
      "\n",
      "We have projects in NY ready to go…\n",
      "\n",
      "RT @CBSNews: \"What happened to that American spirit?\"\n",
      "\n",
      "While visiting Washington, D.C. on Wednesday, New York Gov. Andrew Cuomo slammed the…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original tweets\\n')\n",
    "for t in alltweets[:5]:\n",
    "    print(f'{t.full_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated tweets\n",
      "\n",
      "elvis A project to build an end to end nlp project would be great. Even better if students can collaborate on it.\n",
      "\n",
      "Fabinho is the best defensive midfielder in the world! Flight me #LFC\n",
      "\n",
      "Are we memeing this into existence? I’m in #Mbappe2020\n",
      "\n",
      "Simon Mignolet UEFA Champions League Liverpool FC (at 🏠) You deserve it just as much Simon! Your professionalism is an inspiration.\n",
      "\n",
      "So fucking nervous! Let’s get what Klopp and the boys deserve! #LFC #UCLFinal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Curated tweets\\n')\n",
    "for t in curated_tweets[:5]:\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove boring tweets (tweets with only urls or too short) and cleanup texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_tweet(tweet):\n",
    "    \"Clean tweet text\"\n",
    "    text = ' '.join(t for t in tweet.split() if 'http' not in t)\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    if text.split() and text.split()[0] == '.':\n",
    "         text = ' '.join(text.split()[1:])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boring_tweet(tweet):\n",
    "    \"Check if this is a boring tweet\"\n",
    "    boring_stuff = ['http', '@', '#', 'thank', 'thanks', 'I', 'you']\n",
    "    if len(tweet.split()) < 3:\n",
    "        return True\n",
    "    if all(any(bs in t.lower() for bs in boring_stuff) for t in tweet):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated tweets: 491\n",
      "Cool tweets: 486\n"
     ]
    }
   ],
   "source": [
    "clean_tweets = [cleanup_tweet(t) for t in curated_tweets]\n",
    "cool_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]\n",
    "print(f'Curated tweets: {len(curated_tweets)}\\nCool tweets: {len(cool_tweets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split data into training and validation sets (90/10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "random.shuffle(cool_tweets)\n",
    "\n",
    "# fraction of training data\n",
    "split_train_valid = 0.9\n",
    "\n",
    "# split dataset\n",
    "train_size = int(split_train_valid * len(cool_tweets))\n",
    "valid_size = len(cool_tweets) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(cool_tweets, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export our datasets as text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{handle}_train.txt', 'w') as f:\n",
    "    f.write('\\n'.join(train_dataset))\n",
    "\n",
    "with open(f'{handle}_valid.txt', 'w') as f:\n",
    "    f.write('\\n'.join(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log and monitor training through W&B\n",
    "\n",
    "In order to check our model is training correctly and compare experiments, we are going to use the W&B integration from huggingface.\n",
    "\n",
    "### [Sign up for a free account →](https://app.wandb.ai/login?signup=true)\n",
    "\n",
    "- **Unified dashboard**: Central repository for all your model metrics and predictions\n",
    "- **Lightweight**: No code changes required to integrate with Hugging Face\n",
    "- **Accessible**: Free for individuals and academic teams\n",
    "- **Secure**: All projects are private by default\n",
    "- **Trusted**: Used by machine learning teams at OpenAI, Toyota, GitHub, Lyft and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key\n",
    "Once you've signed up, run the next cell and click on the link to get your API key and authenticate this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "HuggingFace includes the script `run_language_modeling` making it easy to fine-tune a pre-trained model.\n",
    "\n",
    "We use a pre-trained GPT-2 model and fine-tune it on our dataset.\n",
    "\n",
    "Training is automatically logged on W&B (see [documentation](https://docs.wandb.com/huggingface)). Urls are generated to visualize ongoing runs or you can just open your [dashboard](http://app.wandb.ai/).\n",
    "\n",
    "I quickly tested running for several epochs and my run was showing I started overfitting after 4 epochs so this is the limit I use to fine-tune my model (takes less than 2 minutes).\n",
    "\n",
    "![](https://i.imgur.com/1uIxLFe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate run to a project (optional)\n",
    "#%env WANDB_PROJECT=huggingtweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use HuggingFace script `run_language_modeling.py` to fine-tune our model (see [doc](https://huggingface.co/transformers/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/nfs/site/disks/gia_analytics_shared/paddy/projects/git_repos/simpletransformers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20eaefa85fa246268001cd9fb3bc0253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=437.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99756ce0a257429a809b45bc9d79fb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=82.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Saving features into cached file cache_dir/gpt2_cached_lm_126_PradhyumnaR_train.txt\n",
      "INFO:simpletransformers.language_modeling.language_modeling_model: Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92314187166844e9a89069697755692a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=8.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fe4bf5fb2a4d7b9fb103d15091582d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=3.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 5.095858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236a304d568841fc93e49cb157af984f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=3.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.662007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94e8aac74c44a35ac52e389dda773cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=3.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.603417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2147ede79174cce91af6143e7d6a238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=3.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.515154\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98aee4320a8e406898fb8e6b0d4aac5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=3.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.451951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fc22d27d314a7ea7a08a4999f9f489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=3.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.359577\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8356293d834abaa23379a0c44190d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=3.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.257218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76018c7b886e4b709ed56a4ebb3ac6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=3.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.260036\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_model: Training of gpt2 model complete. Saved to gpt2_outputs/.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.language_modeling import LanguageModelingModel\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "train_args = {\n",
    "    \"output_dir\": \"gpt2_outputs/\",\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"fp16\": False,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"eval_batch_size\":32,\n",
    "    \"num_train_epochs\": 8,\n",
    "    \"tensorboard_dir\": 'gpt2_tweet_runs/',\n",
    "    'mlm':False\n",
    "}\n",
    "\n",
    "model = LanguageModelingModel('gpt2', 'gpt2', args=train_args,use_cuda=False)\n",
    "\n",
    "model.train_model(f\"{handle}_train.txt\", eval_file=f\"{handle}_valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['There is no doubt about it. It\\'s not an easy win. I\\'d love to see them win it. I hope they can do it. I\\'ve seen them do that already against Chelsea.\"\\n\\nLiverpool fans will be looking to see if he can score the goal, but it\\'s not the most important game for them, and they need more. Liverpool will win, and they won\\'t have any problems in the second half.\\n\\nLFC fans, it\\'s important to win games. That\\'s what it takes.\\n\\nLFC is not the only club to have an easy season. Arsenal have won the title. I can\\'t see why any club can lose the title. They\\'ve got some good players and a good manager and the squad is strong. I\\'m sure they\\'ll be a lot better next season. I don\\'t know what the hell happened to him but he is a great player. I can\\'t see why he would do this to Liverpool. He\\'s the most talented player']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simpletransformers.language_generation import LanguageGenerationModel\n",
    "gen_args={'length':200,\n",
    "         'k':10}\n",
    "model = LanguageGenerationModel(\"gpt2\", \"gpt2_outputs/\",use_cuda=False, args=gen_args)\n",
    "model.generate(\"There is no\",verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test our trained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test our model on a few sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCES = [\"I think that\",\n",
    "            \"I like\",\n",
    "            \"I don't like\",\n",
    "            \"I want\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use HuggingFace script `run_generation.py` to generate sentences (see [doc](https://huggingface.co/transformers/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = random.randint(0, 2**32-1)\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "\n",
    "for start in SENTENCES:\n",
    "    val = !python run_generation.py \\\n",
    "        --model_type gpt2 \\\n",
    "        --model_name_or_path output/$handle \\\n",
    "        --length 150 \\\n",
    "        --stop_token \"{'\\n'}\" \\\n",
    "        --num_return_sequences 3 \\\n",
    "        --temperature 1 \\\n",
    "        --seed $seed \\\n",
    "        --prompt {'\"' + start + '\"'}\n",
    "    generated = [val[-1-2*k] for k in range(3)[::-1]]\n",
    "    print(f'\\nStart of sentence: {start}')\n",
    "    for i, g in enumerate(generated):\n",
    "        g = g.replace('<|endoftext|>', '')\n",
    "        print(f'* Generated #{i+1}: {g}')\n",
    "        examples.append([start, g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We log the results on our previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve last run\n",
    "project = %env WANDB_PROJECT\n",
    "wandb_id = wandb.api.list_runs(project)[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log results on our previous wandb run\n",
    "wandb.init(id=wandb_id, resume='must')\n",
    "wandb.log({'examples': wandb.Table(data=examples, columns=['Input', 'Prediction'])})\n",
    "\n",
    "# Update display name\n",
    "wandb.run.name = alltweets[0].author.name\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**: Open your generated \"Run page\" generated and look at the predictions in the \"Media\" panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see my trained models on my dashboard.\n",
    "\n",
    "### [W&B Report →](https://bit.ly/2TGXMZf)\n",
    "\n",
    "Please share your experiments and any insights you have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "*Built by Boris Dayma*\n",
    "\n",
    "[![Follow](https://img.shields.io/twitter/follow/borisdayma?style=social)](https://twitter.com/borisdayma)\n",
    "\n",
    "My main goals with this project are:\n",
    "* to experiment with how to train, deploy and maintain neural networks in production ;\n",
    "* to make AI accessible to everyone.\n",
    "\n",
    "To see how the model works, visit the project repository.\n",
    "\n",
    "[![GitHub stars](https://img.shields.io/github/stars/borisdayma/huggingtweets?style=social)](https://github.com/borisdayma/huggingtweets)\n",
    "\n",
    "**Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [A Step by Step Guide to Tracking Hugging Face Model Performance](https://app.wandb.ai/jxmorris12/huggingface-demo/reports/A-Step-by-Step-Guide-to-Tracking-Hugging-Face-Model-Performance--VmlldzoxMDE2MTU)\n",
    "* [W&B Forum](http://bit.ly/wandb-forum): If you have any questions, reach out to the slack community"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
