{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingTweets - Tweet Generation with Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation (NLG).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('credentials.json','r') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download tweets from a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download latest tweets associated to a user account through [Tweepy](http://docs.tweepy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate\n",
    "auth = tweepy.AppAuthHandler(credentials.get('api_key'), credentials.get('api_secret'))\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grab all available tweets (limited to 3200 per API limitations) based on Twitter handle.\n",
    "\n",
    "**Note**: Protected users may only be requested when the authenticated user either \"owns\" the timeline or is an approved follower of the owner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <--- Enter the screen name of the user you will download your dataset from --->\n",
    "handle = 'elonMusk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets = api.user_timeline(\n",
    "        screen_name=handle, tweet_mode='extended', count=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before id 1264278385200517119\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before id 1258644775852322815\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before id 1254632509863866367\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before id 1250614618805821439\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before id 1243274074832371712\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before id 1235671666572668928\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before id 1225352310051758080\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before id 1217742268393607167\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before id 1209217693108318207\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before id 1197987290317914111\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before id 1186369486800113663\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before id 1177662806117584895\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before id 1171802135383564288\n",
      "...2799 tweets downloaded so far\n",
      "getting tweets before id 1163017566576496639\n",
      "...2999 tweets downloaded so far\n",
      "getting tweets before id 1155181226661052415\n",
      "...3199 tweets downloaded so far\n",
      "getting tweets before id 1148119175916732415\n",
      "...3202 tweets downloaded so far\n",
      "getting tweets before id 1148109025273208831\n",
      "...3202 tweets downloaded so far\n",
      "getting tweets before id 1148109025273208831\n",
      "...3202 tweets downloaded so far\n",
      "getting tweets before id 1148109025273208831\n",
      "...3202 tweets downloaded so far\n",
      "getting tweets before id 1148109025273208831\n",
      "...3202 tweets downloaded so far\n",
      "getting tweets before id 1148109025273208831\n",
      "...3202 tweets downloaded so far\n",
      "getting tweets before id 1148109025273208831\n"
     ]
    }
   ],
   "source": [
    "# Adapted from https://gist.github.com/onmyeoin/62c72a7d61fc840b2689b2cf106f583c\n",
    "\n",
    "# initialize a list to hold all the tweepy Tweets & list with no retweets\n",
    "alltweets = []\n",
    "\n",
    "# make initial request for most recent tweets with extended mode enabled to get full tweets\n",
    "new_tweets = api.user_timeline(\n",
    "    screen_name=handle, tweet_mode='extended', count=200)\n",
    "\n",
    "# save most recent tweets\n",
    "alltweets.extend(new_tweets)\n",
    "\n",
    "# save the id of the oldest tweet less one\n",
    "oldest = alltweets[-1].id - 1\n",
    "\n",
    "# check we cannot get more tweets\n",
    "no_tweet_count = 0\n",
    "\n",
    "# keep grabbing tweets until the api limit is reached\n",
    "while True:\n",
    "    print(f'getting tweets before id {oldest}')\n",
    "\n",
    "    # all subsequent requests use the max_id param to prevent duplicates\n",
    "    new_tweets = api.user_timeline(\n",
    "        screen_name=handle, tweet_mode='extended', count=200, max_id=oldest)\n",
    "    \n",
    "    # stop if no more tweets (try a few times as they sometimes eventually come)\n",
    "    if not new_tweets:\n",
    "        no_tweet_count +=1\n",
    "    else:\n",
    "        no_tweet_count = 0\n",
    "    if no_tweet_count > 5: break\n",
    "\n",
    "    # save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    # update the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    print(f'...{len(alltweets)} tweets downloaded so far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Month')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_months = pd.DataFrame({'tweet_month':[vars(status)['created_at'].strftime('%b-%Y') for status in alltweets]})\n",
    "monthly = all_months.groupby('tweet_month').size().reset_index(name='counts')\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.barplot(x=monthly['tweet_month'],y=monthly['counts'],ax=ax)\n",
    "ax.set_ylabel('Num Tweets')\n",
    "ax.set_xlabel('Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset from downloaded tweets\n",
    "\n",
    "We remove:\n",
    "* retweets (since it's not in the wording style of target author)\n",
    "* tweets with no interesting content (limited to url's, user mentionss, \"thank you\"…)\n",
    "\n",
    "We clean up remaining tweets:\n",
    "* we remove url's\n",
    "* we replace \"@\" mentions with user names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_handle:\n",
    "    'Get a user handle and cache it to avoid calling too much twitter api.'\n",
    "    handles = {}\n",
    "    def get(handle):\n",
    "        if handle not in user_handle.handles.keys():            \n",
    "            try:\n",
    "                user_handle.handles[handle] = api.get_user(handle).name\n",
    "            except:\n",
    "                user_handle.handles[handle] = None\n",
    "        return user_handle.handles[handle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_handle(word):\n",
    "    'Replace user handles, remove \"@\" and \"#\"'\n",
    "    if word[0] == '@':\n",
    "        handle = re.search('^@(\\w)+', word)\n",
    "        if handle:\n",
    "            user = user_handle.get(handle.group())\n",
    "            if user is not None: return user + word[handle.endpos:]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_tweet(tweet):\n",
    "    'Return true if not a retweet'\n",
    "    if hasattr(tweet, 'retweeted_status'):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_tweets(tweets):\n",
    "    'Decide which tweets we keep and replace handles'\n",
    "    curated_tweets = []\n",
    "    for tweet in tweets:\n",
    "        if keep_tweet(tweet):\n",
    "            curated_tweets.append(' '.join(replace_handle(w) for w in tweet.full_text.split()))\n",
    "    return curated_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_tweets = curate_tweets(alltweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify our list of tweets is well curated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 3202\n",
      "Curated tweets: 2940\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of tweets: {len(alltweets)}\\nCurated tweets: {len(curated_tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweets\n",
      "\n",
      "RT @SpaceX: More than 100 spacecraft have been signed up to fly on Falcon 9 since we launched the rideshare program. Small satellite operat…\n",
      "\n",
      "RT @tegmark: Help us find an unsung hero! If they win, they get $50k &amp; you get up to $3k for nominating/spreading the word. Our first 3 awa…\n",
      "\n",
      "@NYYScoreKeep Not sure. They’re pretty close together.\n",
      "\n",
      "@westcoastbill It will be real\n",
      "\n",
      "@cisoml @Erdayastronaut @russ_parrish Maybe Incat https://t.co/qHbJcMocFg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original tweets\\n')\n",
    "for t in alltweets[:5]:\n",
    "    print(f'{t.full_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated tweets\n",
      "\n",
      "NYYaker Not sure. They’re pretty close together.\n",
      "\n",
      "bill lee 🚀🇺🇸 It will be real\n",
      "\n",
      "Ciso M Lins Everyday Astronaut Russ Parrish Maybe Incat https://t.co/qHbJcMocFg\n",
      "\n",
      "Viv 🐉 There will be many test flights before commercial passengers are carried. First Earth to Earth test flights might be in 2 or 3 years.\n",
      "\n",
      "Everyday Astronaut Russ Parrish We need to be far enough away so as not to bother heavily populated areas. The launch &amp; landing are not subtle. But you could get within a few miles of the spaceport in a boat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Curated tweets\\n')\n",
    "for t in curated_tweets[:5]:\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove boring tweets (tweets with only urls or too short) and cleanup texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_tweet(tweet):\n",
    "    \"Clean tweet text\"\n",
    "    text = ' '.join(t for t in tweet.split() if 'http' not in t)\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    if text.split() and text.split()[0] == '.':\n",
    "         text = ' '.join(text.split()[1:])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boring_tweet(tweet):\n",
    "    \"Check if this is a boring tweet\"\n",
    "    boring_stuff = ['http', '@', '#', 'thank', 'thanks', 'I', 'you']\n",
    "    if len(tweet.split()) < 3:\n",
    "        return True\n",
    "    if all(any(bs in t.lower() for bs in boring_stuff) for t in tweet):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated tweets: 2940\n",
      "Cool tweets: 2753\n"
     ]
    }
   ],
   "source": [
    "clean_tweets = [cleanup_tweet(t) for t in curated_tweets]\n",
    "cool_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]\n",
    "print(f'Curated tweets: {len(curated_tweets)}\\nCool tweets: {len(cool_tweets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split data into training and validation sets (90/10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "random.shuffle(cool_tweets)\n",
    "\n",
    "# fraction of training data\n",
    "split_train_valid = 0.9\n",
    "\n",
    "# split dataset\n",
    "train_size = int(split_train_valid * len(cool_tweets))\n",
    "valid_size = len(cool_tweets) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(cool_tweets, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./tweets_cache/{handle}_train.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(train_dataset))\n",
    "\n",
    "with open(f\"./tweets_cache/{handle}_valid.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.language_modeling import LanguageModelingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Creating features from dataset file at cache_dir/\n",
      " Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ccde6d4f604420939ef5c7f102a9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f2354831264f9a8734d43aad4c165a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=459.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Saving features into cached file cache_dir/gpt2_cached_lm_126_elonMusk_train.txt\n",
      " Saving features into cached file cache_dir/gpt2_cached_lm_126_elonMusk_train.txt\n",
      "INFO:simpletransformers.language_modeling.language_modeling_model: Training started\n",
      " Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a044562e794c8384285e1ca8bc6a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=8.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41777e747334bf69016219ff0469e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=15.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 5.139063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c948dc3b075a496aba7df381fb12c6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=15.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.793602\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a7c68076374c3586f938ab8be0cd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=15.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.469554\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a55f1dd26343efa58d3e0be9512dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=15.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.455385"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/orto/home/pradhyum/.conda/envs/my_root/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.519169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69da01594d164504b852b3c40583515e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=15.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.079566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba0f706313b4098a40c887701ed337e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=15.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.948889\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a840890be41e44a09d7e4925ae1018a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=15.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.120066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0037916f5e6040dda97f25188eff90ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=15.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.224104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_model: Training of gpt2 model complete. Saved to gpt2_outputs/.\n",
      " Training of gpt2 model complete. Saved to gpt2_outputs/.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "train_args = {\n",
    "    \"output_dir\": \"gpt2_outputs/\",\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"fp16\": False,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"eval_batch_size\":32,\n",
    "    \"num_train_epochs\": 8,\n",
    "    \"tensorboard_dir\": 'gpt2_tweet_runs/',\n",
    "    'mlm':False\n",
    "}\n",
    "\n",
    "model = LanguageModelingModel('gpt2', 'gpt2', args=train_args,use_cuda=False)\n",
    "\n",
    "model.train_model(f\"./tweets_cache/{handle}_train.txt\", eval_file=f\"./tweets_cache/{handle}_valid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test our trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['There is no question that there will be some improvements coming out of the factory in a few months.Tesla Owners Silicon Valley 🇵🇷🛩🔋🔧 @matthewlindy We can’t be done with just the right tools, but there’s a way to get there, and we’re doing our best to solve that problem.John K. Brown🚀 @matthewlindy Tesla is a great place to start a new career. The team is amazing!🤣🤣🤣Tesla Owners Silicon Valley 🐉 Tesla Podcast Everyday Astronaut Astronaut YesJohn Kraus 🚀 Tesla Podcast Yeah! We’re going to use the same technology as our competitors! It’s a great opportunity to test out new concepts, improve our design, & improve our customer experience. That is what we’re working towards.Tesla Owners Silicon Valley 🐉 Tesla Podcast']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simpletransformers.language_generation import LanguageGenerationModel\n",
    "gen_args={'length':200,\n",
    "         'k':10}\n",
    "model = LanguageGenerationModel(\"gpt2\", \"gpt2_outputs/\",use_cuda=False, args=gen_args)\n",
    "model.generate(\"There is no\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"profile = api.get_user(id=handle)\\n    image_url = profile.profile_image_url[:63]+profile.profile_image_url[70:]\\n    urllib.request.urlretrieve(image_url,f'{handle}.jpg')\\n    image = Image.open('sunrise.jpg')\\n    st.image(image)\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''profile = api.get_user(id=handle)\n",
    "    image_url = profile.profile_image_url[:63]+profile.profile_image_url[70:]\n",
    "    urllib.request.urlretrieve(image_url,f'{handle}.jpg')\n",
    "    image = Image.open('sunrise.jpg')\n",
    "    st.image(image)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit to Boris Dayma for the original repo that this is forked from."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
